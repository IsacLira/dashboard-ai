"""Code evaluation agent.

This module provides the CodeEvaluator agent which analyzes and validates
Python code generated by the Analytics Agent.
"""

import json
import re
from typing import Dict, Any
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.prebuilt import create_react_agent

from .base import BaseAgent
from .tools import DataTools


class CodeEvaluator(BaseAgent):
    """Evaluates generated code quality and robustness.
    
    This agent analyzes Python/Pandas code, tests it with edge cases,
    and provides scored feedback with improvement suggestions.
    
    Attributes:
        SYSTEM_PROMPT: The system prompt defining evaluation criteria.
        data_tools: DataTools instance for testing code.
    """
    
    SYSTEM_PROMPT = """Você é um Code Reviewer especializado em análise de dados com Python/Pandas.
Sua missão é avaliar código gerado por outro agente e garantir qualidade e robustez.

CRITÉRIOS DE AVALIAÇÃO (Score 0-100):

1. **CORREÇÃO** (60 pontos):
   - Código executa sem erros? (30pts)
   - Lógica está correta para o objetivo? (15pts)
   - Trata edge cases básicos? (15pts)

2. **QUALIDADE** (40 pontos):
   - Código é eficiente (evita loops desnecessários)? (15pts)
   - Segue boas práticas do Pandas? (15pts)
   - É legível e manutenível? (10pts)

PROCESSO DE AVALIAÇÃO:
1. Analise o código fornecido
2. Use execute_python_analysis para testar execução
3. Calcule o score total baseado nos critérios
4. Gere feedback estruturado

FORMATO DE SAÍDA (JSON):
{
  "score": 85,
  "passed_execution": true,
  "feedback": {
    "strengths": ["Código eficiente usando groupby", "Trata valores nulos com dropna()"],
    "weaknesses": ["Não valida se a coluna existe", "Falha com DataFrame vazio"],
    "suggestions": ["Adicionar verificação: if 'coluna' in df.columns", "Adicionar validação: if not df.empty"]
  },
  "action": "IMPROVE",
  "improved_code": "# Código melhorado aqui (se action=IMPROVE ou REWRITE)"
}

AÇÕES BASEADAS NO SCORE:
- **APPROVE** (score > 80): Código aprovado, sem mudanças necessárias
- **IMPROVE** (60-80): Sugerir melhorias específicas + código melhorado
- **REWRITE** (< 60): Propor solução alternativa completa

IMPORTANTE: Sempre retorne JSON válido. Se não conseguir avaliar, retorne score 0."""
    
    def __init__(self, llm: any, data_tools: DataTools) -> None:
        """Initialize the Code Evaluator.
        
        Args:
            llm: Language model instance.
            data_tools: DataTools instance for code testing.
        """
        self.data_tools = data_tools
        super().__init__(llm, data_tools.get_tools())
    
    def _create_agent(self) -> any:
        """Create the code evaluator agent.
        
        Returns:
            Created LangGraph agent instance.
        """
        return create_react_agent(self.llm, self.tools)
    
    def process(self, query: str) -> str:
        """Process is not used for CodeEvaluator.
        
        Use evaluate() method instead.
        
        Args:
            query: Not used.
            
        Returns:
            Empty string.
        """
        return ""
    
    def evaluate(self, code: str, original_query: str) -> Dict[str, Any]:
        """Evaluate code quality and return scored feedback.
        
        Args:
            code: The Python code to evaluate.
            original_query: The original user query for context.
            
        Returns:
            Dictionary containing score, feedback, and action.
        """
        self.logger.info(
            f"[Code Evaluator] Evaluating code for query: {original_query}"
        )
        
        try:
            evaluation_query = f"""Avalie o seguinte código Python/Pandas:

```python
{code}
```

Query original do usuário: "{original_query}"

Use as tools disponíveis para testar o código e retorne a avaliação em JSON."""
            
            response = self._invoke([
                SystemMessage(content=self.SYSTEM_PROMPT),
                HumanMessage(content=evaluation_query)
            ])
            
            content = self._extract_content(response)
            
            # Try to extract JSON from response
            evaluation = self._parse_evaluation(content)
            
            self.logger.info(
                f"[Code Evaluator] Score: {evaluation.get('score', 0)}, "
                f"Action: {evaluation.get('action', 'UNKNOWN')}"
            )
            
            return evaluation
            
        except Exception as e:
            self.logger.error(f"[Code Evaluator] Error during evaluation: {str(e)}")
            return self._default_evaluation(str(e))
    
    def _parse_evaluation(self, content: str) -> Dict[str, Any]:
        """Parse evaluation JSON from response content.
        
        Args:
            content: The response content potentially containing JSON.
            
        Returns:
            Parsed evaluation dictionary.
        """
        # Look for JSON in the response
        json_match = re.search(r'\{[\s\S]*\}', content)
        
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                self.logger.warning("[Code Evaluator] Invalid JSON in response")
        
        self.logger.warning("[Code Evaluator] Could not extract JSON from response")
        return self._default_evaluation("Avaliação inconclusiva")
    
    def _default_evaluation(self, reason: str) -> Dict[str, Any]:
        """Return default evaluation when parsing fails.
        
        Args:
            reason: Reason for using default evaluation.
            
        Returns:
            Default evaluation dictionary.
        """
        return {
            "score": 50,
            "passed_execution": False,
            "feedback": {"weaknesses": [reason]},
            "action": "APPROVE"  # Default to approve on error
        }
